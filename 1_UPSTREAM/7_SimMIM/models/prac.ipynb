{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "from monai.data import (\n",
    "    Dataset,\n",
    "    Dataset,\n",
    ")\n",
    "import math\n",
    "\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    RandSpatialCropd,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    ScaleIntensityRanged,\n",
    "    ToTensord,\n",
    "    RandAffined,\n",
    "    RandZoomd,\n",
    "    RandRotated\n",
    ")\n",
    "\n",
    "\n",
    "def datafold_read(datalist, basedir, fold=0, key=\"training\"):\n",
    "    with open(datalist) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = json_data[key]\n",
    "\n",
    "    for d in json_data:\n",
    "        for k, v in d.items():\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [os.path.join(basedir, iv) for iv in d[k]]\n",
    "            elif isinstance(d[k], str):\n",
    "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 else d[k]\n",
    "\n",
    "    tr = []\n",
    "    val = []\n",
    "    for d in json_data:\n",
    "        if \"fold\" in d and d[\"fold\"] == fold:\n",
    "            val.append(d)\n",
    "        else:\n",
    "            tr.append(d)\n",
    "    # pdb.set_trace()\n",
    "    return tr, val\n",
    "\n",
    "\n",
    "class MaskGenerator:\n",
    "    def __init__(self, input_size=192, mask_patch_size=32, model_patch_size=(2, 2, 2), mask_ratio=0.6):\n",
    "        self.input_size = input_size #192\n",
    "        self.mask_patch_size = mask_patch_size #32\n",
    "        self.model_patch_size = model_patch_size[0] #2\n",
    "        self.mask_ratio = mask_ratio #0.6\n",
    "        assert self.input_size % self.mask_patch_size == 0\n",
    "        assert self.mask_patch_size % self.model_patch_size == 0\n",
    "        self.rand_size = self.input_size // self.mask_patch_size #6\n",
    "        self.scale = self.mask_patch_size // self.model_patch_size #8\n",
    "        self.token_count = int(self.rand_size*self.rand_size*self.rand_size/2) #6*6*3\n",
    "        self.mask_count = int(np.ceil(self.token_count * self.mask_ratio))\n",
    "\n",
    "    def __call__(self):\n",
    "        mask_idx = np.random.permutation(self.token_count)[: self.mask_count]\n",
    "        mask = np.zeros(self.token_count, dtype=int)\n",
    "        mask[mask_idx] = 1\n",
    "        mask = mask.reshape((self.rand_size, self.rand_size, int(self.rand_size/2)))\n",
    "        mask = mask.repeat(self.scale, axis=0).repeat(self.scale, axis=1).repeat(self.scale, axis=2)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class Transform:\n",
    "    def __init__(self):\n",
    "        self.transform_pet = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\"], image_only=True),\n",
    "                EnsureChannelFirstd(keys=[\"image\"]),\n",
    "                Orientationd(keys=[\"image\"], axcodes=\"LPS\"),\n",
    "                ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=22.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "                ToTensord(keys=[\"image\"], track_meta=False),\n",
    "            ]\n",
    "        )\n",
    "            \n",
    "        self.mask_generator = MaskGenerator()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.transform_pet(img)\n",
    "        mask = self.mask_generator()\n",
    "        return img, mask\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if not isinstance(batch[0][0], tuple):\n",
    "        return default_collate(batch)\n",
    "    else:\n",
    "        batch_num = len(batch)\n",
    "        ret = []\n",
    "        for item_idx in range(len(batch[0][0])):\n",
    "            if batch[0][0][item_idx] is None:\n",
    "                ret.append(None)\n",
    "            else:\n",
    "                ret.append(default_collate([batch[i][0][item_idx] for i in range(batch_num)]))\n",
    "        ret.append(default_collate([batch[i][1] for i in range(batch_num)]))\n",
    "        return ret\n",
    "\n",
    "import natsort\n",
    "import glob\n",
    "\n",
    "def build_loader_simmim():\n",
    "    train_real = natsort.natsorted(glob.glob(f'/workspace/Ablation/ABLATION_PD/RECONSTRUCTION/DATA/*/*.nii.gz'))[:] #ALL -> 2125 or 2130\n",
    "\n",
    "    print(\"Train [Total]  number = \", len(train_real))\n",
    "\n",
    "    files_tr = [{\"image\": tr_img} for tr_img in zip(train_real)]\n",
    "\n",
    "    transform = Transform()\n",
    "    dataset_train = Dataset(data = files_tr, transform = transform)\n",
    "\n",
    "    dataloader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return dataloader_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "import glob\n",
    "path_list = []\n",
    "ori_list = natsort.natsorted(glob.glob(f'/workspace/Ablation/ABLATION_PD/RECONSTRUCTION/DATA/*/*.nii.gz'))\n",
    "for i in range(30):\n",
    "    a_item = natsort.natsorted(glob.glob(f'/workspace/Ablation/ABLATION_PD/RECONSTRUCTION/DATA/*/*.nii.gz'))[i].replace(\"DATA\", \"7_SimMIM\")\n",
    "    path_list.append(a_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "from monai.networks.blocks.dynunet_block import UnetBasicBlock, UnetResBlock, get_conv_layer\n",
    "\n",
    "from swin_transformer_3d import SwinTransformer3D\n",
    "\n",
    "\n",
    "class PixelShuffle3D(nn.Module):\n",
    "    \"\"\"\n",
    "    https://github.com/assassint2017/PixelShuffle3D/blob/master/PixelShuffle3D.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(PixelShuffle3D, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, channels, in_depth, in_height, in_width = inputs.size()\n",
    "        channels //= self.upscale_factor**3\n",
    "        out_depth = in_depth * self.upscale_factor\n",
    "        out_height = in_height * self.upscale_factor\n",
    "        out_width = in_width * self.upscale_factor\n",
    "        input_view = inputs.contiguous().view(\n",
    "            batch_size,\n",
    "            channels,\n",
    "            self.upscale_factor,\n",
    "            self.upscale_factor,\n",
    "            self.upscale_factor,\n",
    "            in_depth,\n",
    "            in_height,\n",
    "            in_width,\n",
    "        )\n",
    "        shuffle_out = input_view.permute(0, 1, 5, 2, 6, 3, 7, 4).contiguous()\n",
    "        return shuffle_out.view(batch_size, channels, out_depth, out_height, out_width)\n",
    "\n",
    "\n",
    "class SwinTransformerForSimMIM(SwinTransformer3D):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        trunc_normal_(self.mask_token, mean=0.0, std=0.02)\n",
    "        self.layers = nn.ModuleList([self.layers1, self.layers2, self.layers3, self.layers4])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        _, _, D, H, W = x.size()\n",
    "        # x = self.patch_embed(x)\n",
    "        patched_x = self.patch_embed(x)\n",
    "        # print(\"patch embed size : \", patched_x.shape)\n",
    "        # x = x.flatten(2).transpose(1, 2)\n",
    "        flattened_patched_x = patched_x.flatten(2).transpose(1, 2)\n",
    "        assert mask is not None\n",
    "        # B, L, _ = x.shape\n",
    "        B, L, _ = flattened_patched_x.shape\n",
    "        mask_tokens = self.mask_token.expand(B, L, -1)\n",
    "        # print(\"mask_tokens.shape :\", mask_tokens.shape)\n",
    "        w = mask.flatten(1).unsqueeze(-1).type_as(mask_tokens)\n",
    "        x = flattened_patched_x * (1.0 - w) + mask_tokens * w\n",
    "        x = self.pos_drop(x)\n",
    "        x = x.view(-1, self.embed_dim, D // self.patch_size[0], H // self.patch_size[1], W // self.patch_size[2])\n",
    "        # print(\"first x_shape :\", x.shape)\n",
    "        x_input = x\n",
    "        for layer in self.layers:\n",
    "            x = layer[0](x)\n",
    "            # print(\"x_shape :\", x.shape)\n",
    "        reduction = self.patch_size[0] * 16\n",
    "        x = x.reshape(-1, (D // reduction) * (H // reduction) * (W // reduction), 2 * self.num_features)\n",
    "        x = self.norm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x.view(-1, 2 * self.num_features, D // 32, H // 32, W // 32)\n",
    "        # print(\"last x_shape :\", x.shape)\n",
    "        return x, flattened_patched_x, mask_tokens, x_input, w\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return super().no_weight_decay() | {\"mask_token\"}\n",
    "\n",
    "class SimMIM(nn.Module):\n",
    "    def __init__(self, encoder, encoder_stride, decoder=\"pixel_shuffle\", loss=\"mask_only\"):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder_stride = encoder_stride\n",
    "        self.decoder = decoder\n",
    "        self.loss = loss\n",
    "\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels=2 * self.encoder.num_features, out_channels=self.encoder_stride**3 * 1, kernel_size=1\n",
    "        )\n",
    "        self.pixel_shuffle = PixelShuffle3D(self.encoder_stride)\n",
    "\n",
    "        self.in_chans = self.encoder.in_chans\n",
    "        self.patch_size = self.encoder.patch_size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        z, flattened_patched_x, mask_tokens, x_input, mask_flatten = self.encoder(x, mask)\n",
    "        x_rec = self.pixel_shuffle(self.conv1(z))\n",
    "\n",
    "        mask = (\n",
    "            mask.repeat_interleave(self.patch_size[0], 1)\n",
    "            .repeat_interleave(self.patch_size[1], 2)\n",
    "            .repeat_interleave(self.patch_size[2], 3)\n",
    "            .unsqueeze(1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        loss_recon = F.l1_loss(x, x_rec, reduction=\"none\")\n",
    "        loss = (loss_recon * mask).sum() / (mask.sum() + 1e-5) / self.in_chans\n",
    "\n",
    "        return loss, flattened_patched_x, mask_tokens, x_input, mask_flatten, x_rec\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        if hasattr(self.encoder, \"no_weight_decay\"):\n",
    "            return {\"encoder.\" + i for i in self.encoder.no_weight_decay()}\n",
    "        return {}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        if hasattr(self.encoder, \"no_weight_decay_keywords\"):\n",
    "            return {\"encoder.\" + i for i in self.encoder.no_weight_decay_keywords()}\n",
    "        return {}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_path : /workspace/Ablation/ABLATION_PD/FINE_TUNING/WEIGHTS/SIMMIM_1900.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/tmp/ipykernel_2581/3773771024.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = f'/workspace/Ablation/ABLATION_PD/FINE_TUNING/WEIGHTS/SIMMIM_1900.pt'\n",
    "print(f\"ckpt_path : {ckpt_path}\")\n",
    "encoder = SwinTransformerForSimMIM(\n",
    "    num_classes=1,#fine tuning시 바꿀 것\n",
    "    img_size=192,\n",
    "    patch_size=(2, 2, 2),\n",
    "    in_chans=1,\n",
    "    embed_dim=48,\n",
    "    depths=[2, 2, 2, 2],\n",
    "    num_heads=[3, 6, 12, 24],\n",
    "    window_size=(7, 7, 7),\n",
    "    mlp_ratio=4.0,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=0.0,\n",
    "    drop_path_rate=0.1,\n",
    "    # use_checkpoint=args.use_grad_checkpoint,\n",
    "    patch_norm=True,\n",
    ")\n",
    "encoder_stride = 32\n",
    "model = SimMIM(encoder=encoder, encoder_stride=encoder_stride)\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "model.load_state_dict(ckpt['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "def save_img(img, save_path):\n",
    "    img = img.cpu().detach().numpy().transpose(0,4,3,2,1).squeeze().astype(np.float32)\n",
    "    save_pred = sitk.GetImageFromArray(img)\n",
    "    sitk.WriteImage(save_pred, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train [Total]  number =  30\n"
     ]
    }
   ],
   "source": [
    "loader = build_loader_simmim()\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (img, mask) in enumerate(loader):\n",
    "        img = img[\"image\"].cuda(non_blocking=True)\n",
    "        mask = mask.cuda(non_blocking=True)\n",
    "        loss, flattened_patched_x, mask_tokens, x_input, mask_flatten, x_rec = model(img, mask)\n",
    "        save_img(x_rec, path_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 192, 192, 96])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Ablation/ABLATION_PD/7_SimMIM/7_SimMIM_DATA/NC/NM_0073_centered_normalized_occipital.nii.gz'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 96, 48])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input[0][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
