{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai import data, transforms\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import natsort\n",
    "import SimpleITK as sitk\n",
    "\n",
    "def get_loader():\n",
    "    train_real = natsort.natsorted(glob.glob(f'/workspace/PD_SSL_ZOO/3_RECONSTRUCTION/DATA/*.nii.gz'))[:] #ALL -> 2125 or 2130\n",
    "\n",
    "    print(\"Train [Total]  number = \", len(train_real))\n",
    "\n",
    "    files_tr = [img_tr for img_tr in zip(train_real)]\n",
    "\n",
    "    tr_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImage(image_only=True),\n",
    "            transforms.EnsureChannelFirst(),\n",
    "            transforms.Orientation(axcodes=\"RAI\"),\n",
    "            transforms.EnsureType(),\n",
    "            transforms.ToTensor(track_meta=False)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # new_dataset -> Cachenew_dataset\n",
    "    train_ds = data.Dataset(data = files_tr, transform = tr_transforms)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=False\n",
    "        # persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    print(\"loader is ver (train)\")\n",
    "\n",
    "    loader = train_loader\n",
    "\n",
    "    return loader, train_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#GAN_INV\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch.nn import Conv2d, BatchNorm2d, Conv3d, BatchNorm3d, PReLU, ReLU, Sigmoid, MaxPool2d, MaxPool3d, AdaptiveAvgPool3d, Sequential, Module\n",
    "\n",
    "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
    "\t\"\"\" A named tuple describing a ResNet block. \"\"\"\n",
    "\n",
    "def get_block(in_channel, depth, num_units, stride=2):\n",
    "\treturn [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n",
    "\n",
    "def get_blocks(num_layers):\n",
    "\tif num_layers == 50:\n",
    "\t\tblocks = [\n",
    "\t\t\tget_block(in_channel=64, depth=64, num_units=3),\n",
    "\t\t\tget_block(in_channel=64, depth=128, num_units=4),\n",
    "\t\t\tget_block(in_channel=128, depth=256, num_units=14),\n",
    "\t\t\tget_block(in_channel=256, depth=512, num_units=3)\n",
    "\t\t]\n",
    "\telif num_layers == 100:\n",
    "\t\tblocks = [\n",
    "\t\t\tget_block(in_channel=64, depth=64, num_units=3),\n",
    "\t\t\tget_block(in_channel=64, depth=128, num_units=13),\n",
    "\t\t\tget_block(in_channel=128, depth=256, num_units=30),\n",
    "\t\t\tget_block(in_channel=256, depth=512, num_units=3)\n",
    "\t\t]\n",
    "\telif num_layers == 152:\n",
    "\t\tblocks = [\n",
    "\t\t\tget_block(in_channel=128, depth=128, num_units=11),\n",
    "\t\t\tget_block(in_channel=128, depth=256, num_units=36),\n",
    "\t\t\tget_block(in_channel=256, depth=512, num_units=3)\n",
    "\t\t]\n",
    "\telse:\n",
    "\t\traise ValueError(\"Invalid number of layers: {}. Must be one of [50, 100, 152]\".format(num_layers))\n",
    "\treturn blocks\n",
    "\n",
    "\n",
    "class SEModule_3D(Module):\n",
    "\tdef __init__(self, channels, reduction):\n",
    "\t\tsuper(SEModule_3D, self).__init__()\n",
    "\t\tself.avg_pool = AdaptiveAvgPool3d(1)\n",
    "\t\tself.fc1 = Conv3d(channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n",
    "\t\tself.relu = ReLU(inplace=True)\n",
    "\t\tself.fc2 = Conv3d(channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n",
    "\t\tself.sigmoid = Sigmoid()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmodule_input = x\n",
    "\t\tx = self.avg_pool(x)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = self.sigmoid(x)\n",
    "\t\treturn module_input * x\n",
    "\n",
    "class bottleneck_IR_SE_3D(Module):\n",
    "\tdef __init__(self, in_channel, depth, stride):\n",
    "\t\tsuper(bottleneck_IR_SE_3D, self).__init__()\n",
    "\t\tif in_channel == depth:\n",
    "\t\t\tself.shortcut_layer = MaxPool3d(1, stride)\n",
    "\t\telse:\n",
    "\t\t\tself.shortcut_layer = Sequential(\n",
    "\t\t\t\tConv3d(in_channel, depth, (1, 1, 1), stride, bias=False),\n",
    "\t\t\t\tBatchNorm3d(depth)\n",
    "\t\t\t)\n",
    "\t\tself.res_layer = Sequential(\n",
    "\t\t\tBatchNorm3d(in_channel),\n",
    "\t\t\tConv3d(in_channel, depth, (3, 3, 3), (1, 1, 1), 1, bias=False),\n",
    "\t\t\tPReLU(depth),\n",
    "\t\t\tConv3d(depth, depth, (3, 3, 3), stride, 1, bias=False),\n",
    "\t\t\tBatchNorm3d(depth),\n",
    "\t\t\tSEModule_3D(depth, 16)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tshortcut = self.shortcut_layer(x)\n",
    "\t\tres = self.res_layer(x)\n",
    "\t\treturn res + shortcut\n",
    "\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, lr_mul = 1, bias = True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "        self.lr_mul = lr_mul\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight * self.lr_mul, bias=self.bias * self.lr_mul)\n",
    "\n",
    "class GradualStyleBlock_3D(Module):\n",
    "    def __init__(self, in_c, out_c, spatial):\n",
    "        super(GradualStyleBlock_3D, self).__init__()\n",
    "        self.out_c = out_c\n",
    "        self.spatial = spatial\n",
    "        num_pools = int(np.log2(spatial*(2/3)))\n",
    "        modules = []\n",
    "        modules += [Conv3d(in_c, out_c, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.LeakyReLU()]\n",
    "        for i in range(num_pools - 1):\n",
    "            modules += [\n",
    "                Conv3d(out_c, out_c, kernel_size=3, stride=2, padding=1),\n",
    "                nn.LeakyReLU()\n",
    "            ]\n",
    "        modules += [\n",
    "            Conv3d(out_c, out_c, kernel_size=3, stride=(2,2,1), padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*modules)\n",
    "        self.linear = EqualLinear(out_c, out_c, lr_mul=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self.out_c)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class GradualStyleEncoder_3D(Module):\n",
    "    def __init__(self, num_layers = 50):\n",
    "        super(GradualStyleEncoder_3D, self).__init__()\n",
    "        blocks = get_blocks(num_layers)\n",
    "\n",
    "        unit_module = bottleneck_IR_SE_3D\n",
    "\n",
    "        self.input_layer = Sequential(Conv3d(1, 64, (3, 3, 3), 1, 1, bias=False),\n",
    "                                      BatchNorm3d(64),\n",
    "                                      PReLU(64))\n",
    "        \n",
    "        modules = []\n",
    "        for block in blocks:\n",
    "            for bottleneck in block:\n",
    "                modules.append(unit_module(bottleneck.in_channel,\n",
    "                                           bottleneck.depth,\n",
    "                                           bottleneck.stride))\n",
    "        self.body = Sequential(*modules)\n",
    "\n",
    "        self.styles = nn.ModuleList()\n",
    "        self.style_count = 12 \n",
    "        self.coarse_ind = 4\n",
    "        self.middle_ind = 8\n",
    "\n",
    "        for i in range(self.style_count):\n",
    "            if i < self.coarse_ind:\n",
    "                style = GradualStyleBlock_3D(512, 512, 12)\n",
    "            elif i < self.middle_ind:\n",
    "                style = GradualStyleBlock_3D(512, 512, 24)\n",
    "            else:\n",
    "                style = GradualStyleBlock_3D(512, 512, 48)\n",
    "            self.styles.append(style)\n",
    "\n",
    "        self.latlayer1 = nn.Conv3d(256, 512, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer2 = nn.Conv3d(128, 512, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        _, _, H, W, D = y.size()\n",
    "        return F.interpolate(x, size=(H, W, D), mode='trilinear', align_corners=True) + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        latents = []\n",
    "        modulelist = list(self.body._modules.values())\n",
    "        for i, l in enumerate(modulelist):\n",
    "            x = l(x)\n",
    "            if i == 6:\n",
    "                c1 = x \n",
    "            elif i == 20:\n",
    "                c2 = x \n",
    "            elif i == 23:\n",
    "                c3 = x \n",
    "\n",
    "        for j in range(self.coarse_ind):\n",
    "            latents.append(self.styles[j](c3))\n",
    "\n",
    "        p2 = self._upsample_add(c3, self.latlayer1(c2))\n",
    "        for j in range(self.coarse_ind, self.middle_ind):\n",
    "            latents.append(self.styles[j](p2))\n",
    "\n",
    "        p1 = self._upsample_add(p2, self.latlayer2(c1))\n",
    "        for j in range(self.middle_ind, self.style_count):\n",
    "            latents.append(self.styles[j](p1))\n",
    "\n",
    "        out = torch.stack(latents, dim=1)\n",
    "        return out\n",
    "\n",
    "class pSp(nn.Module):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(pSp, self).__init__()\n",
    "\t\tself.n_styles = 12\n",
    "\n",
    "\t\t# Define architecture\n",
    "\t\tself.encoder = self.set_encoder()\n",
    "\n",
    "\tdef set_encoder(self):\n",
    "\n",
    "\t\tencoder = GradualStyleEncoder_3D()\n",
    "\n",
    "\t\treturn encoder\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\n",
    "\t\tcodes = self.encoder(x) \n",
    "\n",
    "\t\treturn codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#GAN\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import fire\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from math import floor, log2\n",
    "from random import random\n",
    "from shutil import rmtree\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from contextlib import contextmanager, ExitStack\n",
    "from diff_augment import DiffAugment\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad as torch_grad\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from kornia.filters import filter3d\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from vector_quantize_pytorch import VectorQuantize\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_AVAILABLE = True\n",
    "except:\n",
    "    APEX_AVAILABLE = False\n",
    "\n",
    "import aim\n",
    "\n",
    "assert torch.cuda.is_available(), 'You need to have an Nvidia GPU with CUDA installed.'\n",
    "import sys\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "EXTS = ['jpg', 'jpeg', 'png']\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class NanException(Exception):\n",
    "    pass\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "    def update_average(self, old, new):\n",
    "        if not exists(old):\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, prob, fn, fn_else = lambda x: x):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.fn_else = fn_else\n",
    "        self.prob = prob\n",
    "    def forward(self, x):\n",
    "        fn = self.fn if random() < self.prob else self.fn_else\n",
    "        return fn(x)\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "class ChanNorm(nn.Module): #3D\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = ChanNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "class PermuteToFrom(nn.Module): #3D\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 4, 1)\n",
    "        out, *_, loss = self.fn(x)\n",
    "        out = out.permute(0, 4, 1, 2, 3)\n",
    "        return out, loss\n",
    "\n",
    "class Blur(nn.Module): #3D\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        f = torch.Tensor([1, 2, 1])\n",
    "        self.register_buffer('f', f)\n",
    "    def forward(self, x):\n",
    "        f = self.f\n",
    "        f = f[None, None, :] * f[None, :, None] * f[:, None, None]\n",
    "        f = f[None,:,:,:]\n",
    "        return filter3d(x, f, normalized=True)\n",
    "\n",
    "def save_image(tensor, path):\n",
    "    # array = tensor.cpu().detach()\n",
    "    array = tensor.cpu().detach().numpy().transpose(0,4,3,2,1).squeeze()\n",
    "    array = np.fliplr(array)\n",
    "    img = sitk.GetImageFromArray(array)\n",
    "    sitk.WriteImage(img, path)\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "@contextmanager\n",
    "def null_context():\n",
    "    yield\n",
    "\n",
    "def combine_contexts(contexts):\n",
    "    @contextmanager\n",
    "    def multi_contexts():\n",
    "        with ExitStack() as stack:\n",
    "            yield [stack.enter_context(ctx()) for ctx in contexts]\n",
    "    return multi_contexts\n",
    "\n",
    "def default(value, d):\n",
    "    return value if exists(value) else d\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i\n",
    "\n",
    "def cast_list(el):\n",
    "    return el if isinstance(el, list) else [el]\n",
    "\n",
    "def is_empty(t):\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        return t.nelement() == 0\n",
    "    return not exists(t)\n",
    "\n",
    "def raise_if_nan(t):\n",
    "    if torch.isnan(t):\n",
    "        raise NanException\n",
    "\n",
    "def gradient_accumulate_contexts(gradient_accumulate_every, is_ddp, ddps):\n",
    "    if is_ddp:\n",
    "        num_no_syncs = gradient_accumulate_every - 1\n",
    "        head = [combine_contexts(map(lambda ddp: ddp.no_sync, ddps))] * num_no_syncs\n",
    "        tail = [null_context]\n",
    "        contexts =  head + tail\n",
    "    else:\n",
    "        contexts = [null_context] * gradient_accumulate_every\n",
    "\n",
    "    for context in contexts:\n",
    "        with context():\n",
    "            yield\n",
    "\n",
    "def loss_backwards(fp16, loss, optimizer, loss_id, **kwargs):\n",
    "    if fp16:\n",
    "        with amp.scale_loss(loss, optimizer, loss_id) as scaled_loss:\n",
    "            scaled_loss.backward(**kwargs)\n",
    "    else:\n",
    "        loss.backward(**kwargs)\n",
    "\n",
    "def gradient_penalty(images, output, weight = 10):\n",
    "    batch_size = images.shape[0]\n",
    "    gradients = torch_grad(outputs=output, inputs=images,\n",
    "                           grad_outputs=torch.ones(output.size(), device=images.device),\n",
    "                           create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.reshape(batch_size, -1)\n",
    "    return weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "def calc_pl_lengths(styles, images):\n",
    "    device = images.device\n",
    "    num_pixels = images.shape[2] * images.shape[3] * images.shape[4]\n",
    "    pl_noise = torch.randn(images.shape, device=device) / math.sqrt(num_pixels)\n",
    "    outputs = (images * pl_noise).sum()\n",
    "\n",
    "    pl_grads = torch_grad(outputs=outputs, inputs=styles,\n",
    "                          grad_outputs=torch.ones(outputs.shape, device=device),\n",
    "                          create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    return (pl_grads ** 2).sum(dim=2).mean(dim=1).sqrt()\n",
    "\n",
    "def noise(n, latent_dim, device):\n",
    "    return torch.randn(n, latent_dim).cuda(device)\n",
    "\n",
    "def noise_list(n, layers, latent_dim, device):\n",
    "    return (noise(n, latent_dim, device).unsqueeze(1).repeat([1, layers, 1]))\n",
    "\n",
    "def mixed_list(n, layers, latent_dim, device):\n",
    "    tt = int(torch.rand(()).numpy() * layers)\n",
    "    return noise_list(n, tt, latent_dim, device) + noise_list(n, layers - tt, latent_dim, device)\n",
    "\n",
    "def latent_to_w(style_vectorizer, z, num_conv):\n",
    "    return (style_vectorizer(z).unsqueeze(1).repeat([1,num_conv,1]))\n",
    "\n",
    "def image_noise(n, im_size, device):\n",
    "    return torch.FloatTensor(n, im_size, im_size, int(im_size/2), 1).uniform_(0., 1.).cuda(device)\n",
    "\n",
    "def leaky_relu(p=0.2):\n",
    "    return nn.LeakyReLU(p, inplace=True)\n",
    "\n",
    "def evaluate_in_chunks(max_batch_size, model, *args):\n",
    "    split_args = list(zip(*list(map(lambda x: x.split(max_batch_size, dim=0), args))))\n",
    "    chunked_outputs = [model(*i) for i in split_args]\n",
    "    if len(chunked_outputs) == 1:\n",
    "        return chunked_outputs[0] \n",
    "    return torch.cat(chunked_outputs, dim=0)\n",
    "\n",
    "def styles_def_to_tensor(styles_def):\n",
    "    return torch.cat([t[:, None, :].expand(-1, n, -1) for t, n in styles_def], dim=1)\n",
    "\n",
    "def set_requires_grad(model, bool):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = bool\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    low_norm = low / torch.norm(low, dim=1, keepdim=True)\n",
    "    high_norm = high / torch.norm(high, dim=1, keepdim=True)\n",
    "    omega = torch.acos((low_norm * high_norm).sum(1))\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low + (torch.sin(val * omega) / so).unsqueeze(1) * high\n",
    "    return res\n",
    "\n",
    "# losses\n",
    "\n",
    "def gen_hinge_loss(fake, real):\n",
    "    return fake.mean()\n",
    "\n",
    "def hinge_loss(real, fake):\n",
    "    return (F.relu(1 + real) + F.relu(1 - fake)).mean()\n",
    "\n",
    "def dual_contrastive_loss(real_logits, fake_logits):\n",
    "    device = real_logits.device\n",
    "    real_logits, fake_logits = map(lambda t: rearrange(t, '... -> (...)'), (real_logits, fake_logits))\n",
    "\n",
    "    def loss_half(t1, t2):\n",
    "        t1 = rearrange(t1, 'i -> i ()')\n",
    "        t2 = repeat(t2, 'j -> i j', i = t1.shape[0])\n",
    "        t = torch.cat((t1, t2), dim = -1)\n",
    "        return F.cross_entropy(t, torch.zeros(t1.shape[0], device = device, dtype = torch.long))\n",
    "\n",
    "    return loss_half(real_logits, fake_logits) + loss_half(-fake_logits, -real_logits)\n",
    "\n",
    "# dataset\n",
    "\n",
    "def convert_rgb_to_transparent(image):\n",
    "    if image.mode != 'RGBA':\n",
    "        return image.convert('RGBA')\n",
    "    return image\n",
    "\n",
    "def convert_transparent_to_rgb(image):\n",
    "    if image.mode != 'RGB':\n",
    "        return image.convert('RGB')\n",
    "    return image\n",
    "\n",
    "class expand_greyscale(object):\n",
    "    def __init__(self, transparent):\n",
    "        self.transparent = transparent\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        channels = tensor.shape[0]\n",
    "        num_target_channels = 4 if self.transparent else 3\n",
    "\n",
    "        if channels == num_target_channels:\n",
    "            return tensor\n",
    "\n",
    "        alpha = None\n",
    "        if channels == 1:\n",
    "            color = tensor.expand(3, -1, -1)\n",
    "        elif channels == 2:\n",
    "            color = tensor[:1].expand(3, -1, -1)\n",
    "            alpha = tensor[1:]\n",
    "        else:\n",
    "            raise Exception(f'image with invalid number of channels given {channels}')\n",
    "\n",
    "        if not exists(alpha) and self.transparent:\n",
    "            alpha = torch.ones(1, *tensor.shape[1:], device=tensor.device)\n",
    "\n",
    "        return color if not self.transparent else torch.cat((color, alpha))\n",
    "\n",
    "# augmentations\n",
    "\n",
    "def random_hflip(tensor, prob):\n",
    "    if prob < random():\n",
    "        return tensor\n",
    "    return torch.flip(tensor, dims=(3,))\n",
    "\n",
    "def random_vflip(tensor, prob):\n",
    "    if prob < random():\n",
    "        return tensor\n",
    "    return torch.flip(tensor, dims=(4,))\n",
    "\n",
    "def random_zflip(tensor, prob):\n",
    "    if prob < random():\n",
    "        return tensor\n",
    "    return torch.flip(tensor, dims=(-1,))\n",
    "\n",
    "class AugWrapper(nn.Module):\n",
    "    def __init__(self, D, image_size):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "\n",
    "    def forward(self, images, prob = 0.2, types = [], detach = False):\n",
    "        if random() < prob:\n",
    "            images = random_hflip(images, prob=0.2)\n",
    "            images = random_vflip(images, prob=0.2)\n",
    "            images = random_zflip(images, prob=0.2)\n",
    "            images = DiffAugment(images, types=types)\n",
    "\n",
    "        if detach:\n",
    "            images = images.detach()\n",
    "\n",
    "        return self.D(images)\n",
    "\n",
    "# stylegan2 classes\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, lr_mul = 1, bias = True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "        self.lr_mul = lr_mul\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight * self.lr_mul, bias=self.bias * self.lr_mul)\n",
    "\n",
    "class StyleVectorizer(nn.Module):\n",
    "    def __init__(self, emb, depth, lr_mul = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            layers.extend([EqualLinear(emb, emb, lr_mul), leaky_relu()])\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, dim=1)\n",
    "\n",
    "        return self.net(x)\n",
    "\n",
    "class RGBBlock(nn.Module):\n",
    "    def __init__(self, latent_dim, input_channel, upsample, rgba = False):\n",
    "        super().__init__()\n",
    "        self.input_channel = input_channel\n",
    "        self.to_style = nn.Linear(latent_dim, input_channel)\n",
    "\n",
    "        out_filters = 1\n",
    "        self.conv = Conv3DMod(input_channel, out_filters, 1, demod=False)\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor = 2, mode='trilinear', align_corners=False),\n",
    "            Blur()\n",
    "        ) if upsample else None\n",
    "\n",
    "    def forward(self, x, prev_rgb, istyle):\n",
    "        b, c, d, h, w = x.shape\n",
    "        style = self.to_style(istyle)\n",
    "        x = self.conv(x, style)\n",
    "\n",
    "        if exists(prev_rgb):\n",
    "            x = x + prev_rgb\n",
    "\n",
    "        if exists(self.upsample):\n",
    "            x = self.upsample(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Conv3DMod(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, kernel, demod=True, stride=1, dilation=1, eps = 1e-8, **kwargs):\n",
    "        super().__init__()\n",
    "        self.filters = out_chan\n",
    "        self.demod = demod\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.weight = nn.Parameter(torch.randn((out_chan, in_chan, kernel, kernel, kernel)))\n",
    "        self.eps = eps\n",
    "        nn.init.kaiming_normal_(self.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def _get_same_padding(self, size, kernel, dilation, stride):\n",
    "        return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) // 2\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        b, c, d, h, w = x.shape \n",
    "\n",
    "        w1 = y[:, None, :, None, None, None] \n",
    "        w2 = self.weight[None, :, :, :, :, :]\n",
    "        weights = w2 * (w1 + 1)\n",
    "\n",
    "        if self.demod:\n",
    "            dem = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4, 5), keepdim=True) + self.eps)\n",
    "            weights = weights * dem\n",
    "\n",
    "        x = x.reshape(1, -1, d, h, w)\n",
    "\n",
    "        _, _, *ws = weights.shape\n",
    "        weights = weights.reshape(b * self.filters, *ws)\n",
    "\n",
    "        padding = self._get_same_padding(h, self.kernel, self.dilation, self.stride)\n",
    "        x = F.conv3d(x, weights, padding=padding, groups=b)\n",
    "\n",
    "        x = x.reshape(-1, self.filters, d, h, w)\n",
    "        return x\n",
    "\n",
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, latent_dim, input_channels, filters, upsample = True, upsample_rgb = True, rgba = False, ind=0):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False) if upsample else None\n",
    "        self.ind = ind\n",
    "\n",
    "        self.to_style1 = nn.Linear(latent_dim, input_channels)\n",
    "        self.to_noise1 = nn.Linear(1, filters)\n",
    "        self.conv1 = Conv3DMod(input_channels, filters, 3)\n",
    "        \n",
    "        self.to_style2 = nn.Linear(latent_dim, filters)\n",
    "        self.to_noise2 = nn.Linear(1, filters)\n",
    "        self.conv2 = Conv3DMod(filters, filters, 3)\n",
    "\n",
    "        self.activation = leaky_relu()\n",
    "        self.to_rgb = RGBBlock(latent_dim, filters, upsample_rgb, rgba)\n",
    "\n",
    "    def forward(self, x, prev_rgb, istyle_1, istyle_2, inoise_1, inoise_2):\n",
    "        if exists(self.upsample):\n",
    "            x = self.upsample(x)\n",
    "        inoise_1 = inoise_1[:, :x.shape[2], :x.shape[3], :x.shape[4], :] \n",
    "        inoise_2 = inoise_2[:, :x.shape[2], :x.shape[3], :x.shape[4], :] \n",
    "        noise1 = self.to_noise1(inoise_1).permute((0, 4, 1, 2, 3))\n",
    "        noise2 = self.to_noise2(inoise_2).permute((0, 4, 1, 2, 3))\n",
    "        style1 = self.to_style1(istyle_1)\n",
    "        x = self.conv1(x, style1)\n",
    "        x = self.activation(x + noise1)\n",
    "\n",
    "        style2 = self.to_style2(istyle_2)\n",
    "        x = self.conv2(x, style2)\n",
    "        x = self.activation(x + noise2)\n",
    "        rgb = self.to_rgb(x, prev_rgb, istyle_2)\n",
    "\n",
    "        return x, rgb\n",
    "\n",
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, input_channels, filters, downsample=True, ind=0):\n",
    "        super().__init__()\n",
    "        self.conv_res = nn.Conv3d(input_channels, filters, 1, stride = (2 if downsample else 1))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(input_channels, filters, 3, padding=1),\n",
    "            leaky_relu(),\n",
    "            nn.Conv3d(filters, filters, 3, padding=1),\n",
    "            leaky_relu()\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            Blur(),\n",
    "            nn.Conv3d(filters, filters, 3, padding = 1, stride = 2)\n",
    "        ) if downsample else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.conv_res(x)\n",
    "        x = self.net(x)\n",
    "        if exists(self.downsample):\n",
    "            x = self.downsample(x)\n",
    "        x = (x + res) * (1 / math.sqrt(2))\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, \n",
    "                image_size, \n",
    "                latent_dim, \n",
    "                network_capacity = 16, \n",
    "                transparent = False, \n",
    "                attn_layers = [], \n",
    "                no_const = False, \n",
    "                fmap_max = 512,\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = int(log2(image_size*2/3) - 1) * 2\n",
    "\n",
    "        self.no_const = no_const\n",
    "\n",
    "        if no_const:\n",
    "            self.to_initial_block = nn.ConvTranspose3d(latent_dim, 512, 4, 1, 0, bias=False)\n",
    "        else:\n",
    "            self.initial_block = nn.Parameter(torch.randn((1, 512, 6, 6, 3)))\n",
    "\n",
    "        self.initial_conv = nn.Conv3d(512, 512, 3, padding=1)\n",
    "        self.blocks = nn.ModuleList([])\n",
    "\n",
    "        block_0 = GeneratorBlock(\n",
    "            latent_dim,\n",
    "            input_channels=512,\n",
    "            filters=512,\n",
    "            upsample = False,\n",
    "            upsample_rgb = True,\n",
    "            rgba = transparent,\n",
    "            ind = 0\n",
    "        )\n",
    "\n",
    "        block_1 = GeneratorBlock(\n",
    "            latent_dim,\n",
    "            input_channels=512,\n",
    "            filters=512,\n",
    "            upsample = True,\n",
    "            upsample_rgb = True,\n",
    "            rgba = transparent,\n",
    "            ind = 1\n",
    "        )\n",
    "\n",
    "        block_2 = GeneratorBlock(\n",
    "            latent_dim,\n",
    "            input_channels=512,\n",
    "            filters=512,\n",
    "            upsample = True,\n",
    "            upsample_rgb = True,\n",
    "            rgba = transparent,\n",
    "            ind = 2\n",
    "        )\n",
    "\n",
    "        if self.image_size == 48:\n",
    "\n",
    "            block_3 = GeneratorBlock(\n",
    "                latent_dim,\n",
    "                input_channels=512,\n",
    "                filters=256,\n",
    "                upsample = True,\n",
    "                upsample_rgb = False,\n",
    "                rgba = transparent,\n",
    "                ind = 3\n",
    "            )\n",
    "\n",
    "        elif self.image_size == 96:\n",
    "\n",
    "            block_3 = GeneratorBlock(\n",
    "                latent_dim,\n",
    "                input_channels=512,\n",
    "                filters=256,\n",
    "                upsample = True,\n",
    "                upsample_rgb = True,\n",
    "                rgba = transparent,\n",
    "                ind = 3\n",
    "            )\n",
    "\n",
    "            block_4 = GeneratorBlock(\n",
    "                latent_dim,\n",
    "                input_channels=256,\n",
    "                filters=128,\n",
    "                upsample = True,\n",
    "                upsample_rgb = False,\n",
    "                rgba = transparent,\n",
    "                ind = 4\n",
    "            )\n",
    "\n",
    "        elif self.image_size == 192:\n",
    "            \n",
    "            block_3 = GeneratorBlock(\n",
    "                latent_dim,\n",
    "                input_channels=512,\n",
    "                filters=256,\n",
    "                upsample = True,\n",
    "                upsample_rgb = True,\n",
    "                rgba = transparent,\n",
    "                ind = 3\n",
    "            )\n",
    "\n",
    "            block_4 = GeneratorBlock(\n",
    "                latent_dim,\n",
    "                input_channels=256,\n",
    "                filters=128,\n",
    "                upsample = True,\n",
    "                upsample_rgb = True,\n",
    "                rgba = transparent,\n",
    "                ind = 4\n",
    "            )\n",
    "\n",
    "            block_5 = GeneratorBlock(\n",
    "                latent_dim,\n",
    "                input_channels=128,\n",
    "                filters=16,\n",
    "                upsample = True,\n",
    "                upsample_rgb = False,\n",
    "                rgba = transparent,\n",
    "                ind = 5\n",
    "            )\n",
    "\n",
    "        self.blocks.append(block_0)\n",
    "        self.blocks.append(block_1)\n",
    "        self.blocks.append(block_2)\n",
    "\n",
    "        if self.image_size == 48:\n",
    "           self.blocks.append(block_3)\n",
    "\n",
    "        elif self.image_size == 96:\n",
    "            self.blocks.append(block_3)\n",
    "            self.blocks.append(block_4)\n",
    "\n",
    "        elif self.image_size == 192:\n",
    "            self.blocks.append(block_3)\n",
    "            self.blocks.append(block_4)\n",
    "            self.blocks.append(block_5)\n",
    "\n",
    "    def forward(self, styles, input_noise_1, input_noise_2):\n",
    "        batch_size = styles.shape[0]\n",
    "        image_size = self.image_size\n",
    "\n",
    "        x = self.initial_block.expand(batch_size, -1, -1, -1, -1)\n",
    "\n",
    "        rgb = None\n",
    "        x = self.initial_conv(x)\n",
    "\n",
    "        idx = 0\n",
    "        for block in self.blocks:\n",
    "            x, rgb = block(x, rgb, styles[:,idx,:], styles[:,idx+1,:], input_noise_1, input_noise_2)\n",
    "            idx += 2\n",
    "\n",
    "        return rgb\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 image_size, \n",
    "                 network_capacity = 16, \n",
    "                 fq_layers = [], \n",
    "                 fq_dict_size = 256, \n",
    "                 attn_layers = [], \n",
    "                 transparent = False, \n",
    "                 fmap_max = 512,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        num_layers = int(log2(image_size*2/3) - 1) * 2\n",
    "        num_init_filters = 1\n",
    "        self.image_size = image_size\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        block_0 = DiscriminatorBlock(input_channels=1, filters=64, downsample = True, ind=0)\n",
    "        block_1 = DiscriminatorBlock(input_channels=64, filters=128, downsample = True, ind=1)\n",
    "        block_2 = DiscriminatorBlock(input_channels=128, filters=256, downsample = True, ind=2)\n",
    "        \n",
    "        if self.image_size == 48:\n",
    "            block_3 = DiscriminatorBlock(input_channels=256, filters=512, downsample = False, ind=3)\n",
    "        elif self.image_size == 96:\n",
    "            block_3 = DiscriminatorBlock(input_channels=256, filters=512, downsample = True, ind=3)\n",
    "            block_4 = DiscriminatorBlock(input_channels=512, filters=512, downsample = False, ind=4)\n",
    "        elif self.image_size == 192:\n",
    "            block_3 = DiscriminatorBlock(input_channels=256, filters=512, downsample = True, ind=3)\n",
    "            block_4 = DiscriminatorBlock(input_channels=512, filters=512, downsample = True, ind=4)\n",
    "            block_5 = DiscriminatorBlock(input_channels=512, filters=512, downsample = False, ind=5)\n",
    "        \n",
    "        blocks.append(block_0)\n",
    "        blocks.append(block_1)\n",
    "        blocks.append(block_2)\n",
    "        blocks.append(block_3)\n",
    "        \n",
    "        if self.image_size == 48:\n",
    "            pass\n",
    "        elif self.image_size == 96:\n",
    "            blocks.append(block_4)\n",
    "        elif self.image_size == 192:\n",
    "            blocks.append(block_4)\n",
    "            blocks.append(block_5)\n",
    "\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        chan_last = 512\n",
    "        latent_dim = 6 * 6 * 3 * chan_last\n",
    "\n",
    "        self.final_conv = nn.Conv3d(chan_last, chan_last, 3, padding=1)\n",
    "        self.flatten = Flatten()\n",
    "        self.to_logit = nn.Linear(latent_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, *_ = x.shape\n",
    "        index = 0\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.to_logit(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "class StyleGAN2(nn.Module):\n",
    "    def __init__(self, \n",
    "                image_size, \n",
    "                latent_dim = 512, \n",
    "                fmap_max = 512, \n",
    "                style_depth = 8, \n",
    "                network_capacity = 16, \n",
    "                transparent = False, \n",
    "                fp16 = False, \n",
    "                cl_reg = False, \n",
    "                steps = 1, \n",
    "                lr = 1e-4, \n",
    "                ttur_mult = 2, \n",
    "                fq_layers = [], \n",
    "                fq_dict_size = 256, \n",
    "                attn_layers = [], \n",
    "                no_const = False, \n",
    "                lr_mlp = 0.1, \n",
    "                rank = 0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.steps = steps\n",
    "        self.ema_updater = EMA(0.995)\n",
    "\n",
    "        self.S = StyleVectorizer(latent_dim,\n",
    "                                 style_depth,\n",
    "                                 lr_mul = lr_mlp)\n",
    "\n",
    "        self.G = Generator(image_size, \n",
    "                           latent_dim, \n",
    "                           network_capacity, \n",
    "                           transparent = transparent, \n",
    "                           attn_layers = attn_layers, \n",
    "                           no_const = no_const, \n",
    "                           fmap_max = fmap_max,\n",
    "                           )\n",
    "\n",
    "        self.D = Discriminator(image_size, \n",
    "                               network_capacity, \n",
    "                               fq_layers = fq_layers, \n",
    "                               fq_dict_size = fq_dict_size, \n",
    "                               attn_layers = attn_layers, \n",
    "                               transparent = transparent,\n",
    "                               fmap_max = fmap_max,\n",
    "                               )\n",
    "\n",
    "        self.SE = StyleVectorizer(latent_dim, \n",
    "                                  style_depth, \n",
    "                                  lr_mul = lr_mlp)\n",
    "\n",
    "        self.GE = Generator(image_size, \n",
    "                            latent_dim, \n",
    "                            network_capacity, \n",
    "                            transparent = transparent, \n",
    "                            attn_layers = attn_layers, \n",
    "                            no_const = no_const)\n",
    "\n",
    "        self.D_cl = None\n",
    "\n",
    "        # wrapper for augmenting all images going into the discriminator\n",
    "        self.D_aug = AugWrapper(self.D, image_size)\n",
    "\n",
    "        # turn off grad for exponential moving averages\n",
    "        set_requires_grad(self.SE, False)\n",
    "        set_requires_grad(self.GE, False)\n",
    "\n",
    "        # init optimizers\n",
    "        generator_params = list(self.G.parameters()) + list(self.S.parameters())\n",
    "        self.G_opt = Adam(generator_params, lr = self.lr, betas=(0.5, 0.9))\n",
    "        self.D_opt = Adam(self.D.parameters(), lr = self.lr * ttur_mult, betas=(0.5, 0.9))\n",
    "\n",
    "        # init weights\n",
    "        self._init_weights()\n",
    "        self.reset_parameter_averaging()\n",
    "\n",
    "        self.cuda(rank)\n",
    "\n",
    "        # startup apex mixed precision\n",
    "        self.fp16 = fp16\n",
    "        if fp16:\n",
    "            (self.S, self.G, self.D, self.SE, self.GE), (self.G_opt, self.D_opt) = amp.initialize([self.S, self.G, self.D, self.SE, self.GE], [self.G_opt, self.D_opt], opt_level='O1', num_losses=3)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) in {nn.Conv3d, nn.Linear}:\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "        for block in self.G.blocks:\n",
    "            nn.init.zeros_(block.to_noise1.weight)\n",
    "            nn.init.zeros_(block.to_noise2.weight)\n",
    "            nn.init.zeros_(block.to_noise1.bias)\n",
    "            nn.init.zeros_(block.to_noise2.bias)\n",
    "\n",
    "    def EMA(self):\n",
    "        def update_moving_average(ma_model, current_model):\n",
    "            for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "                old_weight, up_weight = ma_params.data, current_params.data\n",
    "                ma_params.data = self.ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "        update_moving_average(self.SE, self.S)\n",
    "        update_moving_average(self.GE, self.G)\n",
    "\n",
    "    def reset_parameter_averaging(self):\n",
    "        self.SE.load_state_dict(self.S.state_dict())\n",
    "        self.GE.load_state_dict(self.G.state_dict())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader, train_list = get_loader()\n",
    "ckpt_enc = torch.load('/workspace/PD_SSL_ZOO/2_DOWNSTREAM/WEIGHTS/4_P2S2P.pt', map_location='cpu')\n",
    "ckpt_dec = torch.load('//workspace/PD_SSL_ZOO/2_DOWNSTREAM/WEIGHTS/4_StyleGAN.pt', map_location='cpu')\n",
    "encoder = GradualStyleEncoder_3D()\n",
    "encoder.load_state_dict(ckpt_enc['state_dict'])\n",
    "dec_model = StyleGAN2(image_size=192)\n",
    "dec_model.load_state_dict(ckpt_dec[\"GAN\"])\n",
    "decoder = dec_model.GE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.to(\"cuda\")\n",
    "decoder.to(\"cuda\")\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "def image_noise(n, im_size, device):\n",
    "    return torch.FloatTensor(n, im_size, im_size, int(im_size/2), 1).uniform_(0., 1.).cuda(device)\n",
    "\n",
    "def save_image(pred, path):\n",
    "    pred_img = pred.cpu().detach().numpy().transpose(0,4,3,2,1).squeeze()\n",
    "    pred_img = np.flipud(pred_img)\n",
    "    pred_img = np.fliplr(pred_img)\n",
    "    pred_img = np.flip(pred_img, axis=2)\n",
    "\n",
    "    save_pred = sitk.GetImageFromArray(pred_img)\n",
    "    sitk.WriteImage(save_pred, path)\n",
    "    \n",
    "input_noise_1 = image_noise(1, 192, 'cuda')\n",
    "input_noise_2 = image_noise(1, 192, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(loader):\n",
    "        x = batch.to(\"cuda\")\n",
    "        codes = encoder(x)\n",
    "        pred = decoder(codes, input_noise_1, input_noise_2)\n",
    "        pred_path = train_list[idx].replace(\"DATA\", \"4_Pixel2Style2Pixel\")\n",
    "        save_image(pred, pred_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
